#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Enhanced AI Manager
Gerenciador de IA com suporte a ferramentas e busca ativa
"""

import os
import logging
import asyncio
import json
from typing import Dict, List, Optional, Any, Union
from datetime import datetime

# Imports condicionais
try:
    import google.generativeai as genai
    from google.generativeai.types import FunctionDeclaration, Tool
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False

try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    from groq import Groq
    HAS_GROQ = True
except ImportError:
    HAS_GROQ = False

# Adicionando suporte ao OpenRouter
try:
    import openai as openrouter_openai
    HAS_OPENROUTER = True
except ImportError:
    HAS_OPENROUTER = False

logger = logging.getLogger(__name__)

class EnhancedAIManager:
    """Gerenciador de IA aprimorado com ferramentas de busca ativa"""

    def __init__(self):
        """Inicializa o gerenciador aprimorado"""
        self.providers = {}
        self.current_provider = None
        self.search_orchestrator = None

        self._initialize_providers()
        self._initialize_search_tools()

        logger.info(f"ü§ñ Enhanced AI Manager inicializado com {len(self.providers)} provedores")

    def _initialize_providers(self):
        """Inicializa todos os provedores de IA"""

        # Qwen via OpenRouter (Prioridade 1 - mais confi√°vel)
        if HAS_OPENROUTER:
            api_key = os.getenv("OPENROUTER_API_KEY")
            if api_key:
                try:
                    openrouter_client = openrouter_openai.OpenAI(
                        api_key=api_key,
                        base_url="https://openrouter.ai/api/v1"
                    )
                    self.providers["openrouter"] = {
                        "client": openrouter_client,
                        "model": "qwen/qwen2.5-vl-32b-instruct:free",
                        "available": True,
                        "supports_tools": False, # Ajuste se o modelo suportar tools
                        "priority": 1
                    }
                    logger.info("‚úÖ Qwen via OpenRouter configurado")
                except Exception as e:
                    logger.error(f"‚ùå Erro ao configurar Qwen/OpenRouter: {e}")

        # Gemini (Prioridade 2)
        if HAS_GEMINI:
            api_key = os.getenv("GEMINI_API_KEY")
            if api_key:
                try:
                    genai.configure(api_key=api_key)
                    self.providers["gemini"] = {
                        "client": genai,
                        "model": "gemini-2.0-flash-exp",
                        "available": True,
                        "supports_tools": True,
                        "priority": 2
                    }
                    logger.info("‚úÖ Gemini 2.0 Flash configurado")
                except Exception as e:
                    logger.error(f"‚ùå Erro ao configurar Gemini: {e}")

        # Groq (Prioridade 3 - fallback confi√°vel) - ATUALIZADO PARA MODELO SUPORTADO
        if HAS_GROQ:
            api_key = os.getenv("GROQ_API_KEY")
            if api_key:
                try:
                    self.providers["groq"] = {
                        "client": Groq(api_key=api_key),
                        "model": "llama3-70b-8192", # Modelo atualizado - veja a tabela de deprecia√ß√µes
                        "available": True,
                        "supports_tools": False,
                        "priority": 3
                    }
                    logger.info("‚úÖ Groq Llama configurado")
                except Exception as e:
                    logger.error(f"‚ùå Erro ao configurar Groq: {e}")

        # OpenAI (Prioridade 4)
        if HAS_OPENAI:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                try:
                    self.providers["openai"] = {
                        "client": openai.OpenAI(api_key=api_key),
                        "model": "gpt-4o",
                        "available": True, # Habilitado
                        "supports_tools": True,
                        "priority": 4
                    }
                    logger.info("‚úÖ OpenAI GPT-4o configurado")
                except Exception as e:
                    logger.error(f"‚ùå Erro ao configurar OpenAI: {e}")

    def _initialize_search_tools(self):
        """Inicializa ferramentas de busca"""
        try:
            from services.real_search_orchestrator import real_search_orchestrator
            self.search_orchestrator = real_search_orchestrator
            logger.info("‚úÖ Ferramentas de busca ativa configuradas")
        except ImportError:
            logger.warning("‚ö†Ô∏è Search orchestrator n√£o dispon√≠vel")

    def _get_best_provider(self, require_tools: bool = False) -> Optional[str]:
        """Seleciona o melhor provedor dispon√≠vel"""
        available = []

        for name, provider in self.providers.items():
            if not provider["available"]:
                continue

            # Se requer tools, pula provedores que n√£o suportam
            if require_tools and not provider.get("supports_tools", False):
                 # Mas permite Qwen mesmo sem tools como fallback se necess√°rio
                 if name != "openrouter": # Qwen pode ser usado mesmo sem tools se for o √∫nico
                    continue

            available.append((name, provider["priority"]))

        if available:
            # Ordena pela prioridade (menor n√∫mero = maior prioridade)
            available.sort(key=lambda x: x[1])
            return available[0][0]

        # Se nenhum provedor com tools est√° dispon√≠vel, mas precisamos de tools
        # Retorna o melhor provedor sem tools
        if require_tools:
            logger.warning("‚ö†Ô∏è Nenhum provedor com tools dispon√≠vel, usando provedor simples")
            return self._get_best_provider(require_tools=False)

        return None

    async def generate_with_active_search(
        self,
        prompt: str,
        context: str = "",
        session_id: str = None,
        max_search_iterations: int = 3,
        study_time_minutes: int = 5
    ) -> str:
        """
        Gera conte√∫do com busca ativa - IA pode buscar informa√ß√µes online
        """
        logger.info(f"üîç Iniciando gera√ß√£o com busca ativa - Tempo de estudo: {study_time_minutes} min")
        
        # FASE DE ESTUDO PROFUNDO
        if context and len(context) > 10000:  # Se h√° muito contexto para estudar
            logger.info(f"üìö INICIANDO FASE DE ESTUDO PROFUNDO - {study_time_minutes} minutos")
            study_start = datetime.now()
            
            # Divide o contexto em chunks para an√°lise profunda
            chunk_size = 8000
            context_chunks = [context[i:i+chunk_size] for i in range(0, len(context), chunk_size)]
            
            study_insights = []
            for i, chunk in enumerate(context_chunks[:10]):  # M√°ximo 10 chunks
                logger.info(f"üìñ Analisando chunk {i+1}/{min(len(context_chunks), 10)}")
                
                study_prompt = f"""
                AN√ÅLISE PROFUNDA E APRENDIZADO:
                
                Analise profundamente este conte√∫do e extraia:
                1. Insights √∫nicos e padr√µes ocultos
                2. Tend√™ncias emergentes
                3. Oportunidades n√£o √≥bvias
                4. Conex√µes entre diferentes informa√ß√µes
                5. Previs√µes baseadas nos dados
                
                CONTE√öDO PARA AN√ÅLISE:
                {chunk}
                
                Seja extremamente anal√≠tico e perspicaz. V√° al√©m do √≥bvio.
                """
                
                try:
                    insight = await self.generate_text(study_prompt)
                    if insight and len(insight) > 100:
                        study_insights.append(insight)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro na an√°lise do chunk {i+1}: {e}")
            
            # Consolida insights do estudo
            if study_insights:
                consolidated_study = "\n\n".join(study_insights)
                context = f"{context}\n\nINSIGHTS DO ESTUDO PROFUNDO:\n{consolidated_study}"
                
            study_duration = (datetime.now() - study_start).total_seconds() / 60
            logger.info(f"‚úÖ Estudo profundo conclu√≠do em {study_duration:.1f} minutos")

        # Tenta Qwen/OpenRouter primeiro para gera√ß√£o com busca ativa
        if "openrouter" in self.providers and self.providers["openrouter"]["available"]:
             provider_name = "openrouter"
             logger.info(f"ü§ñ Usando {provider_name} com busca ativa (priorit√°rio)")
        else:
            # Caso contr√°rio, usa a l√≥gica padr√£o
            provider_name = self._get_best_provider(require_tools=True)
            if not provider_name:
                logger.warning("‚ö†Ô∏è Nenhum provedor com ferramentas dispon√≠vel - usando fallback")
                return await self.generate_text(prompt + "\n\n" + context)

        provider = self.providers[provider_name]
        logger.info(f"ü§ñ Usando {provider_name} com busca ativa")

        # Prepara prompt com instru√ß√µes de busca
        enhanced_prompt = f"""
{prompt}

CONTEXTO DISPON√çVEL:
{context}

INSTRU√á√ïES ESPECIAIS:
- Analise o contexto fornecido detalhadamente
- Busque dados atualizados sobre o mercado brasileiro
- Procure por estat√≠sticas, tend√™ncias e casos reais
- Forne√ßa insights profundos baseados nos dados dispon√≠veis

IMPORTANTE: Gere uma an√°lise completa mesmo sem ferramentas de busca, baseando-se no contexto fornecido.
"""

        try:
            # Executa gera√ß√£o com ferramentas
            if provider_name == "gemini":
                return await self._generate_gemini_with_tools(enhanced_prompt, max_search_iterations, session_id)
            elif provider_name == "openai":
                return await self._generate_openai_with_tools(enhanced_prompt, max_search_iterations, session_id)
            else:
                # Para Qwen/OpenRouter e outros, usa gera√ß√£o simples
                return await self.generate_text(enhanced_prompt)
        except Exception as e:
            logger.error(f"‚ùå Erro com {provider_name}: {e}")
            # Fallback para gera√ß√£o simples com Qwen/OpenRouter
            logger.info("üîÑ Usando fallback para Qwen/OpenRouter")
            return await self.generate_text(enhanced_prompt)

    async def _generate_gemini_with_tools(
        self,
        prompt: str,
        max_iterations: int,
        session_id: str = None
    ) -> str:
        """Gera com Gemini usando ferramentas"""

        try:
            model = genai.GenerativeModel("gemini-2.0-flash-exp")

            # Define fun√ß√£o de busca
            search_function = FunctionDeclaration(
                name="google_search",
                description="Busca informa√ß√µes atualizadas na internet",
                parameters={
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Termo de busca"
                        }
                    },
                    "required": ["query"]
                }
            )

            tool = Tool(function_declarations=[search_function])

            # Inicia chat com ferramentas
            chat = model.start_chat(tools=[tool])

            iteration = 0
            conversation_history = []

            while iteration < max_iterations:
                iteration += 1
                logger.info(f"üîÑ Itera√ß√£o {iteration}/{max_iterations}")

                try:
                    # Envia mensagem
                    if iteration == 1:
                        response = chat.send_message(prompt)
                    else:
                        # Continua conversa com resultados de busca
                        response = chat.send_message("Continue a an√°lise com os dados obtidos.")

                    # Verifica se h√° function calls
                    if response.candidates[0].content.parts:
                        for part in response.candidates[0].content.parts:
                            if part.function_call:
                                function_call = part.function_call

                                if function_call.name == "google_search":
                                    search_query = function_call.args.get("query", "")
                                    logger.info(f"üîç IA solicitou busca: {search_query}")

                                    # Executa busca real
                                    search_results = await self._execute_real_search(search_query, session_id)

                                    # Envia resultados de volta para a IA
                                    search_response = chat.send_message(
                                        f"Resultados da busca para \'{search_query}\':\n{search_results}"
                                    )

                                    conversation_history.append({
                                        "search_query": search_query,
                                        "search_results": search_results[:1000] # Limita para log
                                    })

                                    continue

                    # Se chegou aqui, √© resposta final
                    final_response = response.text

                    logger.info(f"‚úÖ Gera√ß√£o com busca ativa conclu√≠da em {iteration} itera√ß√µes")
                    logger.info(f"üîç {len(conversation_history)} buscas realizadas")

                    return final_response

                except Exception as e:
                    logger.error(f"‚ùå Erro na itera√ß√£o {iteration}: {e}")
                    break

            # Se chegou ao limite de itera√ß√µes
            logger.warning(f"‚ö†Ô∏è Limite de itera√ß√µes atingido ({max_iterations})")
            return "An√°lise realizada com busca ativa, mas processo limitado por itera√ß√µes."

        except Exception as e:
            logger.error(f"‚ùå Erro no Gemini com ferramentas: {e}")
            raise

    async def _generate_openai_with_tools(
        self,
        prompt: str,
        max_iterations: int,
        session_id: str = None
    ) -> str:
        """Gera com OpenAI usando ferramentas"""

        try:
            client = self.providers["openai"]["client"]

            # Define fun√ß√£o de busca
            tools = [{
                "type": "function",
                "function": {
                    "name": "google_search",
                    "description": "Busca informa√ß√µes atualizadas na internet",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "Termo de busca"
                            }
                        },
                        "required": ["query"]
                    }
                }
            }]

            messages = [{"role": "user", "content": prompt}]
            iteration = 0

            while iteration < max_iterations:
                iteration += 1
                logger.info(f"üîÑ Itera√ß√£o OpenAI {iteration}/{max_iterations}")

                try:
                    response = client.chat.completions.create(
                        model=self.providers["openai"]["model"],
                        messages=messages,
                        tools=tools,
                        tool_choice="auto",
                        max_tokens=4000
                    )

                    message = response.choices[0].message

                    # Verifica tool calls
                    if hasattr(message, "tool_calls") and message.tool_calls:
                        tool_call = message.tool_calls[0]

                        if tool_call.function.name == "google_search":
                            args = json.loads(tool_call.function.arguments)
                            search_query = args.get("query", "")

                            logger.info(f"üîç IA OpenAI solicitou busca: {search_query}")

                            # Executa busca real
                            search_results = await self._execute_real_search(search_query, session_id)

                            # Adiciona √† conversa
                            messages.append({
                                "role": "assistant",
                                "tool_calls": [{
                                    "id": tool_call.id,
                                    "type": "function",
                                    "function": {
                                        "name": "google_search",
                                        "arguments": tool_call.function.arguments
                                    }
                                }]
                            })

                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": search_results
                            })

                            continue

                    # Resposta final
                    final_response = message.content
                    logger.info(f"‚úÖ OpenAI gera√ß√£o conclu√≠da em {iteration} itera√ß√µes")
                    return final_response

                except Exception as e:
                    error_msg = str(e)
                    if "429" in error_msg or "quota" in error_msg.lower() or "insufficient_quota" in error_msg.lower():
                        logger.error(f"‚ùå OpenAI quota excedida: {e}")
                        # Marca OpenAI como indispon√≠vel temporariamente
                        self.providers["openai"]["available"] = False
                        logger.info("üîÑ Marcando OpenAI como indispon√≠vel e tentando outro provedor")

                        # Tenta usar outro provedor como fallback
                        fallback_provider = self._get_best_provider(require_tools=False)
                        if fallback_provider and fallback_provider != "openai":
                            logger.info(f"üîÑ Usando {fallback_provider} como fallback para OpenAI")
                            return await self.generate_text(prompt)
                        else:
                            return "OpenAI quota excedida e nenhum provedor alternativo dispon√≠vel. Por favor, configure uma chave API v√°lida."
                    else:
                        logger.error(f"‚ùå Erro na itera√ß√£o OpenAI {iteration}: {e}")
                    break

            return "An√°lise realizada com OpenAI e busca ativa."

        except Exception as e:
            logger.error(f"‚ùå Erro no OpenAI com ferramentas: {e}")
            raise

    async def _execute_real_search(self, search_query: str, session_id: str = None) -> str:
        """Executa busca real usando o orquestrador"""

        if not self.search_orchestrator:
            return f"Busca n√£o dispon√≠vel para: {search_query}"

        try:
            # Executa busca massiva real
            search_results = await self.search_orchestrator.execute_massive_real_search(
                query=search_query,
                context={"ai_requested": True},
                session_id=session_id or "ai_search"
            )

            # Formata resultados para a IA
            formatted_results = self._format_search_results_for_ai(search_results)

            return formatted_results

        except Exception as e:
            logger.error(f"‚ùå Erro na busca real: {e}")
            return f"Erro na busca para \'{search_query}\': {str(e)}"

    def _format_search_results_for_ai(self, search_results: Dict[str, Any]) -> str:
        """Formata resultados de busca para consumo da IA"""

        formatted = """
RESULTADOS DA BUSCA REAL:
Query: {query}
Fontes encontradas: {total_sources}

""".format(
            query=search_results.get("query", ""),
            total_sources=search_results.get("statistics", {}).get("total_sources", 0)
        )

        # Web results
        web_results = search_results.get("web_results", [])
        if web_results:
            formatted += "=== RESULTADOS WEB ===\n"
            for i, result in enumerate(web_results[:10], 1):
                formatted += f"{i}. {result.get('title', 'Sem t√≠tulo')}\n"
                formatted += f"   URL: {result.get('url', '')}\n"
                formatted += f"   Resumo: {result.get('snippet', '')[:200]}...\n\n"

        # YouTube results
        youtube_results = search_results.get("youtube_results", [])
        if youtube_results:
            formatted += "=== RESULTADOS YOUTUBE ===\n"
            for i, result in enumerate(youtube_results[:5], 1):
                formatted += f"{i}. {result.get('title', 'Sem t√≠tulo')}\n"
                formatted += f"   Canal: {result.get('channel', '')}\n"
                formatted += f"   Views: {result.get('view_count', 0):,}\n"
                formatted += f"   Likes: {result.get('like_count', 0):,}\n\n"

        # Social results
        social_results = search_results.get("social_results", [])
        if social_results:
            formatted += "=== RESULTADOS REDES SOCIAIS ===\n"
            for i, result in enumerate(social_results[:5], 1):
                formatted += f"{i}. {result.get('title', 'Sem t√≠tulo')}\n"
                formatted += f"   Plataforma: {result.get('platform', '')}\n"
                formatted += f"   Engajamento: {result.get('viral_score', 0):.1f}/10\n\n"

        # Conte√∫do viral
        viral_content = search_results.get("viral_content", [])
        if viral_content:
            formatted += "=== CONTE√öDO VIRAL ===\n"
            for i, content in enumerate(viral_content[:5], 1):
                formatted += f"{i}. {content.get('title', 'Sem t√≠tulo')}\n"
                formatted += f"   URL: {content.get('url', '')}\n"
                formatted += f"   Plataforma: {content.get('platform', '')}\n"
                formatted += f"   Viral Score: {content.get('viral_score', 0):.1f}/10\n\n"

        # Screenshots
        screenshots = search_results.get("screenshots_captured", [])
        if screenshots:
            formatted += "=== SCREENSHOTS CAPTURADOS ===\n"
            for i, screenshot_path in enumerate(screenshots[:5], 1):
                formatted += f"{i}. {screenshot_path}\n"
            formatted += "\n"

        return formatted

    # M√©todo dummy para 'generate_text' caso seja chamado sem provedor com tools
    async def generate_text(self, prompt: str, max_tokens: int = 4000, temperature: float = 0.7) -> str:
        """Gera texto usando o melhor provedor dispon√≠vel"""
        provider_name = self._get_best_provider(require_tools=False)

        if not provider_name:
            logger.warning("‚ö†Ô∏è Nenhum provedor dispon√≠vel")
            return "Erro: Nenhum provedor de IA dispon√≠vel para gerar texto."

        provider = self.providers[provider_name]
        logger.info(f"ü§ñ Usando {provider_name} para gera√ß√£o de texto")

        try:
            if provider_name == "openrouter":
                client = provider["client"]
                response = client.chat.completions.create(
                    model=provider["model"],
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content

            elif provider_name == "gemini":
                model = genai.GenerativeModel("gemini-2.0-flash-exp")
                response = model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        max_output_tokens=max_tokens,
                        temperature=temperature,
                    )
                )
                return response.text

            elif provider_name == "groq":
                client = provider["client"]
                response = client.chat.completions.create(
                    model=provider["model"],
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content

            elif provider_name == "openai":
                client = provider["client"]
                response = client.chat.completions.create(
                    model=provider["model"],
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content

        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de texto com {provider_name}: {e}")
            return f"Erro na gera√ß√£o: {str(e)}"

        return "Erro: M√©todo de gera√ß√£o n√£o implementado para este provedor"

    async def conduct_deep_study_phase(
        self,
        massive_data: Dict[str, Any],
        session_id: str,
        study_duration_minutes: int = 5
    ) -> Dict[str, Any]:
        """
        ETAPA 2: Conduz estudo profundo de 5 minutos nos dados massivos
        A IA se torna expert no assunto analisando o JSON gigante
        """
        
        logger.info(f"üß† ETAPA 2 - ESTUDO PROFUNDO IA iniciado - Dura√ß√£o: {study_duration_minutes} minutos")
        logger.info(f"üìä Analisando {len(json.dumps(massive_data, ensure_ascii=False))/1024:.1f}KB de dados")
        
        import time
        start_time = time.time()
        study_end_time = start_time + (study_duration_minutes * 60)
        
        # Estrutura de conhecimento expert
        expert_knowledge = {
            "study_metadata": {
                "session_id": session_id,
                "study_start": datetime.now().isoformat(),
                "target_duration_minutes": study_duration_minutes,
                "data_size_analyzed_kb": len(json.dumps(massive_data, ensure_ascii=False)) / 1024,
                "ai_provider_used": self.current_provider
            },
            "domain_expertise": {},
            "market_intelligence": {},
            "competitive_analysis": {},
            "behavioral_insights": {},
            "trend_analysis": {},
            "predictive_insights": {},
            "strategic_recommendations": {},
            "study_phases_completed": []
        }
        
        # Fases de estudo progressivo
        study_phases = [
            ("An√°lise Estrutural dos Dados", self._analyze_data_structure),
            ("Extra√ß√£o de Insights de Mercado", self._extract_market_insights),
            ("An√°lise Competitiva Profunda", self._analyze_competitive_landscape),
            ("Padr√µes Comportamentais", self._identify_behavioral_patterns),
            ("An√°lise de Tend√™ncias", self._analyze_trends),
            ("Gera√ß√£o de Insights Preditivos", self._generate_predictive_insights),
            ("S√≠ntese Estrat√©gica", self._synthesize_strategic_recommendations)
        ]
        
        phase_duration = (study_duration_minutes * 60) / len(study_phases)
        
        for i, (phase_name, phase_function) in enumerate(study_phases):
            if time.time() >= study_end_time:
                logger.warning(f"‚è∞ Tempo limite atingido na fase {i+1}")
                break
                
            logger.info(f"üìö Fase {i+1}/{len(study_phases)}: {phase_name}")
            phase_start = time.time()
            
            try:
                # Executa fase com timeout
                phase_result = await asyncio.wait_for(
                    phase_function(massive_data, expert_knowledge),
                    timeout=phase_duration + 30  # Buffer de 30s
                )
                
                expert_knowledge["study_phases_completed"].append({
                    "phase": phase_name,
                    "completed": True,
                    "duration_seconds": time.time() - phase_start,
                    "insights_generated": len(str(phase_result))
                })
                
                logger.info(f"‚úÖ {phase_name} conclu√≠da em {time.time() - phase_start:.1f}s")
                
            except asyncio.TimeoutError:
                logger.warning(f"‚è∞ Timeout na fase {phase_name}")
                expert_knowledge["study_phases_completed"].append({
                    "phase": phase_name,
                    "completed": False,
                    "timeout": True
                })
            except Exception as e:
                logger.error(f"‚ùå Erro na fase {phase_name}: {e}")
                expert_knowledge["study_phases_completed"].append({
                    "phase": phase_name,
                    "completed": False,
                    "error": str(e)
                })
        
        # Finaliza estudo
        total_study_time = time.time() - start_time
        expert_knowledge["study_metadata"]["actual_duration_seconds"] = total_study_time
        expert_knowledge["study_metadata"]["study_end"] = datetime.now().isoformat()
        expert_knowledge["study_metadata"]["phases_completed"] = len([p for p in expert_knowledge["study_phases_completed"] if p.get("completed")])
        expert_knowledge["study_metadata"]["efficiency_score"] = (expert_knowledge["study_metadata"]["phases_completed"] / len(study_phases)) * 100
        
        logger.info(f"üéì ETAPA 2 conclu√≠da em {total_study_time/60:.1f} minutos")
        logger.info(f"üìä Fases completadas: {expert_knowledge['study_metadata']['phases_completed']}/{len(study_phases)}")
        
        return expert_knowledge

    async def _analyze_data_structure(self, massive_data: Dict[str, Any], expert_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa estrutura dos dados coletados"""
        
        prompt = f"""
        Analise a estrutura dos seguintes dados massivos coletados e extraia insights sobre:
        1. Qualidade e densidade das informa√ß√µes
        2. Principais fontes de dados identificadas
        3. Gaps ou lacunas nos dados
        4. Oportunidades de an√°lise mais profunda
        
        Dados para an√°lise (primeiros 3000 caracteres):
        {str(massive_data)[:3000]}...
        
        Forne√ßa uma an√°lise estrutural concisa e insights acion√°veis.
        """
        
        analysis = await self.generate_text(prompt, max_tokens=1000)
        expert_knowledge["domain_expertise"]["data_structure_analysis"] = analysis
        return {"analysis": analysis}

    async def _extract_market_insights(self, massive_data: Dict[str, Any], expert_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai insights de mercado dos dados"""
        
        market_data = massive_data.get("market_intelligence", {})
        web_data = massive_data.get("web_intelligence", {})
        
        prompt = f"""
        Com base nos dados de mercado coletados, extraia insights profundos sobre:
        1. Tamanho e potencial do mercado
        2. Principais tend√™ncias identificadas
        3. Oportunidades de neg√≥cio
        4. Amea√ßas e desafios
        5. Segmenta√ß√£o de mercado
        
        Dados de mercado:
        {str(market_data)[:2000]}
        
        Dados web relevantes:
        {str(web_data)[:2000]}
        
        Forne√ßa insights estrat√©gicos acion√°veis.
        """
        
        insights = await self.generate_text(prompt, max_tokens=1200)
        expert_knowledge["market_intelligence"] = insights
        return {"insights": insights}

    async def _analyze_competitive_landscape(self, massive_data: Dict[str, Any], expert_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa paisagem competitiva"""
        
        competitive_data = massive_data.get("competitive_intelligence", {})
        
        prompt = f"""
        Analise a paisagem competitiva com base nos dados coletados:
        1. Principais competidores identificados
        2. For√ßas e fraquezas de cada competidor
        3. Gaps competitivos e oportunidades
        4. Estrat√©gias de diferencia√ß√£o recomendadas
        5. Posicionamento ideal no mercado
        
        Dados competitivos:
        {str(competitive_data)[:2500]}
        
        Forne√ßa an√°lise competitiva estrat√©gica.
        """
        
        analysis = await self.generate_text(prompt, max_tokens=1200)
        expert_knowledge["competitive_analysis"] = analysis
        return {"analysis": analysis}

    async def _identify_behavioral_patterns(self, massive_data: Dict[str, Any], expert_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Identifica padr√µes comportamentais"""
        
        behavioral_data = massive_data.get("behavioral_intelligence", {})
        social_data = massive_data.get("social_intelligence", {})
        
        prompt = f"""
        Identifique padr√µes comportamentais dos consumidores:
        1. Principais motiva√ß√µes e necessidades
        2. Pontos de dor identificados
        3. Jornada do cliente t√≠pica
        4. Gatilhos de decis√£o de compra
        5. Canais de comunica√ß√£o preferidos
        
        Dados comportamentais:
        {str(behavioral_data)[:2000]}
        
        Dados sociais:
        {str(social_data)[:2000]}
        
        Forne√ßa insights comportamentais profundos.
        """
        
        patterns = await self.generate_text(prompt, max_tokens=1200)
        expert_knowledge["behavioral_insights"] = patterns
        return {"patterns": patterns}

    async def _analyze_trends(self, massive_data: Dict[str, Any], expert_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa tend√™ncias identificadas"""
        
        trend_data = massive_data.get("trend_intelligence", {})
        
        prompt = f"""
        Analise as tend√™ncias identificadas nos dados:
        1. Tend√™ncias emergentes mais relevantes
        2. Velocidade de ado√ß√£o das tend√™ncias
        3. Impacto potencial no mercado
        4. Oportunidades de capitaliza√ß√£o
        5. Riscos de n√£o acompanhar as tend√™ncias
        
        Dados de tend√™ncias:
        {str(trend_data)[:2500]}
        
        Forne√ßa an√°lise de tend√™ncias estrat√©gica.
        """
        
        analysis = await self.generate_text(prompt, max_tokens=1200)
        expert_knowledge["trend_analysis"] = analysis
        return {"analysis": analysis}

    async def _generate_predictive_insights(self, massive_data: Dict[str, Any], expert_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Gera insights preditivos"""
        
        all_insights = {
            "market": expert_knowledge.get("market_intelligence", ""),
            "competitive": expert_knowledge.get("competitive_analysis", ""),
            "behavioral": expert_knowledge.get("behavioral_insights", ""),
            "trends": expert_knowledge.get("trend_analysis", "")
        }
        
        prompt = f"""
        Com base em toda a an√°lise realizada, gere insights preditivos:
        1. Cen√°rios futuros mais prov√°veis (6-24 meses)
        2. Oportunidades emergentes a serem exploradas
        3. Riscos futuros a serem mitigados
        4. Recomenda√ß√µes de timing para a√ß√µes
        5. Indicadores-chave para monitoramento
        
        S√≠ntese das an√°lises realizadas:
        {str(all_insights)[:3000]}
        
        Forne√ßa previs√µes estrat√©gicas acion√°veis.
        """
        
        predictions = await self.generate_text(prompt, max_tokens=1500)
        expert_knowledge["predictive_insights"] = predictions
        return {"predictions": predictions}

    async def _synthesize_strategic_recommendations(self, massive_data: Dict[str, Any], expert_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Sintetiza recomenda√ß√µes estrat√©gicas finais"""
        
        all_knowledge = {
            "market": expert_knowledge.get("market_intelligence", ""),
            "competitive": expert_knowledge.get("competitive_analysis", ""),
            "behavioral": expert_knowledge.get("behavioral_insights", ""),
            "trends": expert_knowledge.get("trend_analysis", ""),
            "predictions": expert_knowledge.get("predictive_insights", "")
        }
        
        prompt = f"""
        Sintetize todas as an√°lises em recomenda√ß√µes estrat√©gicas finais:
        1. Top 5 prioridades estrat√©gicas imediatas
        2. Plano de a√ß√£o para pr√≥ximos 90 dias
        3. Investimentos recomendados
        4. M√©tricas de sucesso a acompanhar
        5. Pr√≥ximos passos espec√≠ficos
        
        Todo o conhecimento adquirido:
        {str(all_knowledge)[:4000]}
        
        Forne√ßa recomenda√ß√µes estrat√©gicas definitivas e acion√°veis.
        """
        
        recommendations = await self.generate_text(prompt, max_tokens=1500)
        expert_knowledge["strategic_recommendations"] = recommendations
        return {"recommendations": recommendations}


# Inst√¢ncia global
enhanced_ai_manager = EnhancedAIManager()
